{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from os.path import basename, isfile, join, splitext\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from insightface_func.face_detect_crop_single import Face_detect_crop\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from os.path import basename, exists, isfile, join, splitext\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import onnxruntime\n",
    "from util.videoswap import lower_video_resolution, extract_audio, get_frames_n, _totensor\n",
    "\n",
    "import warnings\n",
    "onnxruntime.set_default_logger_severity(3)\n",
    "torch.nn.Module.dump_patches = True\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from face_parsing.bisenet import BiSeNet\n",
    "\n",
    "\n",
    "seg_model = BiSeNet(n_classes=19)\n",
    "seg_model.cuda()\n",
    "save_pth = os.path.join('weights', '79999_iter.pth')\n",
    "seg_model.load_state_dict(torch.load(save_pth))\n",
    "seg_model.eval()\n",
    "\n",
    "\n",
    "model, app = None, None\n",
    "transformer_Arcface = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "opt = TestOptions()\n",
    "opt.initialize()\n",
    "opt.parser.add_argument('-f')  # dummy arg to avoid bug\n",
    "opt = opt.parse()\n",
    "opt.Arc_path = './weights/arcface_checkpoint.tar'\n",
    "opt.isTrain = False\n",
    "torch.nn.Module.dump_patches = True\n",
    "global model\n",
    "model = create_model(opt)\n",
    "model.eval()\n",
    "global app\n",
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id=0, det_thresh=0.6, det_size=(256, 256))\n",
    "\n",
    "# source = '../reference_videos/gen_0.jpg'\n",
    "# target = '../reference_videos/stocks/man_2.mp4'\n",
    "source = '../reference_videos/gen_1.jpg'\n",
    "target = 'IMG_1269.MOV'\n",
    "result_dir='./output'\n",
    "crop_size=224\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "\n",
    "assert isfile(source), f'Can\\'t find source at {source}'\n",
    "assert isfile(target), f'Can\\'t find target at {target}'\n",
    "output_filename = f'infer-{splitext(basename(source))[0]}-{splitext(basename(target))[0]}.mp4'\n",
    "output_path = join(result_dir, output_filename)\n",
    "\n",
    "assert model is not None\n",
    "assert app is not None\n",
    "\n",
    "img_a_whole = cv2.imread(source)\n",
    "img_a_align_crop, _ = app.get(img_a_whole, crop_size)\n",
    "img_a_align_crop_pil = Image.fromarray(\n",
    "    cv2.cvtColor(img_a_align_crop[0], cv2.COLOR_BGR2RGB))\n",
    "img_a = transformer_Arcface(img_a_align_crop_pil)\n",
    "img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])\n",
    "img_id = img_id.cuda()\n",
    "\n",
    "img_id_downsample = F.interpolate(img_id, scale_factor=0.5)\n",
    "latend_id = model.netArc(img_id_downsample)\n",
    "latend_id = latend_id.detach().to('cpu')\n",
    "latend_id = latend_id / np.linalg.norm(latend_id, axis=1, keepdims=True)\n",
    "latend_id = latend_id.to('cuda')\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from fsr.models.SRGAN_model import SRGANModel\n",
    "import easydict\n",
    "\n",
    "esrgan_fsr_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                 transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                      std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'gpu_ids': None,\n",
    "    'batch_size': 32,\n",
    "    'lr_G': 1e-4,\n",
    "    'weight_decay_G': 0,\n",
    "    'beta1_G': 0.9,\n",
    "    'beta2_G': 0.99,\n",
    "    'lr_D': 1e-4,\n",
    "    'weight_decay_D': 0,\n",
    "    'beta1_D': 0.9,\n",
    "    'beta2_D': 0.99,\n",
    "    'lr_scheme': 'MultiStepLR',\n",
    "    'niter': 100000,\n",
    "    'warmup_iter': -1,\n",
    "    'lr_steps': [50000],\n",
    "    'lr_gamma': 0.5,\n",
    "    'pixel_criterion': 'l1',\n",
    "    'pixel_weight': 1e-2,\n",
    "    'feature_criterion': 'l1',\n",
    "    'feature_weight': 1,\n",
    "    'gan_type': 'ragan',\n",
    "    'gan_weight': 5e-3,\n",
    "    'D_update_ratio': 1,\n",
    "    'D_init_iters': 0,\n",
    "\n",
    "    'print_freq': 100,\n",
    "    'val_freq': 1000,\n",
    "    'save_freq': 10000,\n",
    "    'crop_size': 0.85,\n",
    "    'lr_size': 128,\n",
    "    'hr_size': 512,\n",
    "\n",
    "    # network G\n",
    "    'which_model_G': 'RRDBNet',\n",
    "    'G_in_nc': 3,\n",
    "    'out_nc': 3,\n",
    "    'G_nf': 64,\n",
    "    'nb': 16,\n",
    "\n",
    "    # network D\n",
    "    'which_model_D': 'discriminator_vgg_128',\n",
    "    'D_in_nc': 3,\n",
    "    'D_nf': 64,\n",
    "\n",
    "    # data dir\n",
    "    'pretrain_model_G': 'weights/90000_G.pth',\n",
    "    'pretrain_model_D': None\n",
    "})\n",
    "\n",
    "\n",
    "esrgan_fsr_model = SRGANModel(args, is_train=False)\n",
    "esrgan_fsr_model.load()\n",
    "esrgan_fsr_model.netG.to('cuda')\n",
    "esrgan_fsr_model.netG.eval();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# atts = [1 'skin', 2 'l_brow', 3 'r_brow', 4 'l_eye', 5 'r_eye', 6 'eye_g', 7 'l_ear', 8 'r_ear', 9 'ear_r',\n",
    "# 10 'nose', 11 'mouth', 12 'u_lip', 13 'l_lip', 14 'neck', 15 'neck_l', 16 'cloth', 17 'hair', 18 'hat']\n",
    "\n",
    "\n",
    "def reverse2wholeimage(swaped_imgs, mats, crop_size, oriimg, save_path=''):\n",
    "    target_image_list = []\n",
    "    img_mask_list = []\n",
    "    for swaped_img, mat in zip(swaped_imgs, mats):\n",
    "        print('swaped_img:'); plt.imshow(swaped_img.cpu().detach().numpy().transpose((1, 2, 0))); plt.show() ### \n",
    "\n",
    "        swaped_img_ready = F.interpolate(swaped_img.unsqueeze(0), size=(512,512))\n",
    "        seg_mask_logits = seg_model(swaped_img_ready)[0]\n",
    "        seg_mask_logits = F.interpolate(seg_mask_logits, size=(crop_size, crop_size))\n",
    "        seg_mask = seg_mask_logits.squeeze().cpu().detach().numpy().argmax(0).astype(np.uint8)\n",
    "        face_part_ids = [1, 2, 3, 4, 5, 6, 10]\n",
    "        mouth_ids = [11, 12, 13]\n",
    "        img_mask = np.zeros_like(seg_mask)\n",
    "        img_mask[np.isin(seg_mask, face_part_ids)] = 255\n",
    "        \n",
    "        print('img_mask:'); plt.imshow(img_mask); plt.show() ###\n",
    "        print(img_mask.shape, img_mask.min(), img_mask.max())\n",
    "\n",
    "        img_mouth = np.zeros([seg_mask.shape[0], seg_mask.shape[1]])\n",
    "        img_mouth[seg_mask == 11] = 255\n",
    "        img_mouth[seg_mask == 12] = 255\n",
    "        img_mouth[seg_mask == 13] = 255\n",
    "        print('img_mouth:'); plt.imshow(img_mouth); plt.show() ###\n",
    "        print(img_mouth.shape, img_mouth.min(), img_mouth.max())\n",
    "\n",
    "        # select and fill the biggest contour (in case of face hair)\n",
    "        contours, _ = cv2.findContours(img_mask.astype(np.uint8), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        img_mask_ = np.zeros_like(img_mask)\n",
    "        cv2.drawContours(img_mask_, [max(contours, key = cv2.contourArea)], 0, 255, -1)\n",
    "        img_mask = np.array(img_mask_)\n",
    "        img_mask[np.isin(seg_mask, mouth_ids)] = 0\n",
    "        print('img_mask:'); plt.imshow(img_mask); plt.show() ###\n",
    "        print(img_mask.shape, img_mask.min(), img_mask.max())\n",
    "\n",
    "        # median blur to smooth sharp edges\n",
    "        img_mask = cv2.medianBlur(img_mask.astype(np.uint8), 15)\n",
    "        # dilate face region\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "        img_mask = cv2.dilate(img_mask, kernel, iterations=1)\n",
    "        # make dilated region transparent to the border \n",
    "        blur = cv2.GaussianBlur(img_mask, (35, 35), 0, 0)\n",
    "        img_mask = rescale_intensity(blur, in_range=(127.5,255), out_range=(0,255))\n",
    "\n",
    "        # SR: ESRGAN (https://github.com/ewrfcas/Face-Super-Resolution)\n",
    "        swaped_img = esrgan_fsr_transform(torch.clone(swaped_img))\n",
    "        swaped_img = esrgan_fsr_model.netG(swaped_img.unsqueeze(0))\n",
    "        swaped_img = swaped_img.squeeze(0).cpu().detach().numpy().transpose((1, 2, 0))\n",
    "        swaped_img = np.clip(swaped_img / 2.0 + 0.5, 0, 1)\n",
    "\n",
    "        mat_rev = cv2.invertAffineTransform(mat)\n",
    "        mat_rev_face = np.array(mat_rev)\n",
    "        mat_rev_face[:2, :2] = mat_rev_face[:2, :2] / (swaped_img.shape[0] / crop_size)\n",
    "        orisize = (oriimg.shape[1], oriimg.shape[0])\n",
    "\n",
    "        target_image = cv2.warpAffine(swaped_img, mat_rev_face, orisize)\n",
    "        target_image = np.array(target_image, dtype=np.float)[..., ::-1] * 255\n",
    "        target_image_list.append(target_image)\n",
    "\n",
    "        img_mask = cv2.warpAffine(img_mask / 255, mat_rev, orisize)\n",
    "        img_mask = np.reshape(img_mask, [img_mask.shape[0], img_mask.shape[1], 1])\n",
    "        img_mask_list.append(img_mask)\n",
    "\n",
    "    img = np.array(oriimg, dtype=np.float64)\n",
    "    for img_mask, target_image in zip(img_mask_list, target_image_list):\n",
    "        img = img_mask * target_image + (1-img_mask) * img\n",
    "\n",
    "    final_img = img.astype(np.uint8)\n",
    "    print('final_img-RGB:'); plt.imshow(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)); plt.show() ###\n",
    "\n",
    "    cv2.imwrite(save_path, final_img)\n",
    "\n",
    "\n",
    "video_path = target\n",
    "temp_results_dir='./temp_results'\n",
    "swap_model = model\n",
    "detect_model = app\n",
    "id_veсtor = latend_id\n",
    "\n",
    "lower_video_resolution(video_path)\n",
    "print(f'=> Swapping face in \"{video_path}\"...')\n",
    "if exists(temp_results_dir):\n",
    "    shutil.rmtree(temp_results_dir)\n",
    "os.makedirs(temp_results_dir)\n",
    "\n",
    "audio_path = join(temp_results_dir, splitext(basename(video_path))[0] + '.wav')\n",
    "extract_audio(video_path, audio_path)\n",
    "\n",
    "frame_count = get_frames_n(video_path)\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "for i, frame_index in tqdm(enumerate(range(frame_count))): \n",
    "    _, frame = video.read()\n",
    "    if frame_index != 216: #169 - forehead # 216 - mouth\n",
    "        continue\n",
    "    detect_results = detect_model.get(frame, crop_size)     \n",
    "\n",
    "    if detect_results is not None:\n",
    "        frame_align_crop_list = detect_results[0]\n",
    "        frame_mat_list = detect_results[1]\n",
    "        swap_result_list = []\n",
    "\n",
    "        for frame_align_crop in frame_align_crop_list:\n",
    "            frame_align_crop_tensor = _totensor(cv2.cvtColor(frame_align_crop,cv2.COLOR_BGR2RGB))[None,...].cuda()\n",
    "\n",
    "            swap_result = swap_model(None, frame_align_crop_tensor, id_veсtor, None, True)[0]\n",
    "            swap_result_list.append(swap_result)\n",
    "        reverse2wholeimage(swap_result_list, frame_mat_list, crop_size, frame, join(temp_results_dir, 'frame_{:0>7d}.jpg'.format(frame_index)))\n",
    "    else:\n",
    "        frame = frame.astype(np.uint8)\n",
    "        cv2.imwrite(join(temp_results_dir, 'frame_{:0>7d}.jpg'.format(frame_index)), frame)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}